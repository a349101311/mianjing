# softmax、cross entropy

## 一、熵的概念

熵在物理和信息论中都是一个重要的概念，用来衡量一个分布的无序程度。从机器学习角度，概率和信息论中熵的定义：

H(x) = E[- log 2 (P(X))] = -P(x) log 2 (P(X)) I项求和，最大值为P(X) = 0.5时，熵值为1。无序程度越高，熵值越大

## 二、softmax

softmax的功能是将卷积神经网络计算后的多个神经元输出映射到（0,1）区间，给出每种分类的概率。

将输出转化为概率之后，为了衡量输出结果的好坏，就需要选择误差函数。

## 三、cross entropy

交叉熵描述了两个概率分布之间的距离，当交叉熵越小说明二者之间越接近。

交叉熵描述的是两个概率分布之间的距离，但是神经网络的输出却并不一定是一个概率分布。为此，可以通过上述的softmax回归将神经网络的输出值变成概率分布。

交叉熵损失的公式是  - 求和（ground truth 概率 * log（网络预测概率））。

交叉熵越小，损失越低，两个概率分布越接近。

是一个凸优化问题，用梯度下降法求解时，凸优化问题有很好的收敛性。

### tips：one hot

one-hot编码，又称独热编码、一位有效编码。其方法是使用N为状态寄存器来对N个状态进行编码，每个状态都有它独立的寄存器位，并且在任意时候，其中只有一个位有效。

优点是解决了分类器不好处理离散数据的问题，二是在一定程度上也起到了扩充特征的作用。

缺点是在文本特征表示上，它是一个词袋模型，不考虑词与词之间的关系。其次，它假设词之间是独立的（大多数情况下，词与词之间是相互影响的）；最后，它得到的特征是离散稀疏的。

### tips：TF-IDF

TF词频

IDF衡量词的常见程度，逆文档频率

TF-IDF = TF * IDF

优点：简单快速，比较符合实际。

缺点：单纯考虑词频，忽略了词与词之间的位置信息以及词与词之间的相互关系。

## 四、softmax 损失函数

softmax的损失其实就是一个值，而不像交叉熵那种是多个值的叠加，这个值就是真实类别对应的输出概率的导数取log。

