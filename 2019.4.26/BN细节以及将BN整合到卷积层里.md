# BN细节以及将BN整合到卷积层里

## 一、为什么需要BN

在网络训练时，前面层训练参数的更新将导致后面层输入数据分布的变化，我们把网络中间层在训练过程中，数据分布的改变称之为"Internal Covariate Shift"。BN解决的就是这个。

## 二、详解BN

BN与激活函数层、卷积层、全连接层、池化层一样也属于网络的一层。

BN就是对中间层所得到的结果在输入下一层的时候进行一个归一化处理，这个操作一般在卷积操作后，激活函数处理前，不放在激活函数后是因为，通过卷积操作后数据大概率还是符合高斯分布的，好归一化，但是经过非线性操作后就不是那么好归一化了。但是每一层都这么做难免会破坏数据本身的分布（太死板了都是0均值1方差）。所以最重要的一个操作是引入了变换重构，即引入了可学习参数γ和β，分别是标准差和均值。

![1556289003808](C:\Users\zhangqi\AppData\Roaming\Typora\typora-user-images\1556289003808.png)

## 三、将BN整合到Conv层中

BN层是对每个神经元的输入做归一化处理，甚至有时候只需要对某一个神经元进行归一化，而不是对一整层网络的神经元进行归一化。那么在CNN中的卷积层要怎么处理呢，如果feature map 上神经元个数太多，参数γ和β的数量太大了。那么就使用了类似权值共享的策略，把一整张特征图当做一个神经元进行处理。这就是相当于求取所有样本所对应的一个特征图的所有神经元的平均值、方差，然后对这个特征图神经元做归一化。

由于BN在网络前向传播时多了一些层的运算，影响了模型性能，且占用了更多的内存和显存空间。因此很有必要将BN层的参数合并到卷积层来提高模型前向推进的速度。

合并数学原理：https://www.jianshu.com/p/e042d693f3fb

## 四、BN的作用

1. 改善流经网络的梯度。
2. 允许更大的学习率，大幅度提高训练速度。
3. 减少了对权重初始化的依赖。
4. 改善了正则化策略，减少了对正则化的需求，因为BN具有提高网络泛化能力的特性。
5. 可以把训练数据打乱，论文说可以提高1%的精度  

## 五、测试时BN的均值和标准差

均值和方差都是固定的，均值是训练时候得到的每个batch均值的平均值，标准差采用的是每个batch的无偏估计









